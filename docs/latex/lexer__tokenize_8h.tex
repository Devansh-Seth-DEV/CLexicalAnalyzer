\doxysection{/\+Users/devanshseth/\+Library/\+Mobile Documents/com\texorpdfstring{$\sim$}{\string~}apple\texorpdfstring{$\sim$}{\string~}\+Cloud\+Docs/\+Compiler\+Design/\+Lexical Analyzer/headers/lexer\+\_\+tokenize.h File Reference}
\hypertarget{lexer__tokenize_8h}{}\label{lexer__tokenize_8h}\index{/Users/devanshseth/Library/Mobile Documents/com\texorpdfstring{$\sim$}{\string~}apple\texorpdfstring{$\sim$}{\string~}CloudDocs/CompilerDesign/Lexical Analyzer/headers/lexer\_tokenize.h@{/Users/devanshseth/Library/Mobile Documents/com\texorpdfstring{$\sim$}{\string~}apple\texorpdfstring{$\sim$}{\string~}CloudDocs/CompilerDesign/Lexical Analyzer/headers/lexer\_tokenize.h}}


Tokenization core implementation.  


{\ttfamily \#include "{}lexer.\+h"{}}\newline
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
size\+\_\+t \mbox{\hyperlink{group___tokenization_gae0c0a7a1181c1edc46cd937176f4c37c}{tokcnt}} (const char \texorpdfstring{$\ast$}{*}const line)
\begin{DoxyCompactList}\small\item\em Counts the number of tokens in a given string (or file content). \end{DoxyCompactList}\item 
void \mbox{\hyperlink{group___tokenization_ga743b8789dd6718aae4459fbd0b5cba95}{toknz\+\_\+segtoset}} (\mbox{\hyperlink{structtokset__t}{tokset\+\_\+t}} \texorpdfstring{$\ast$}{*}const set, const size\+\_\+t token\+\_\+index, const char \texorpdfstring{$\ast$}{*}const line, const size\+\_\+t start, const size\+\_\+t end, const size\+\_\+t line\+\_\+no, const \mbox{\hyperlink{group___token_classification_ga15a4f5fa9017a6c10fa9e79e2b4cd982}{tokcat\+\_\+e}} category, const size\+\_\+t column)
\begin{DoxyCompactList}\small\item\em Tokenizes a segment of a line and stores the resulting token in the token set. \end{DoxyCompactList}\item 
\mbox{\hyperlink{structtokset__t}{tokset\+\_\+t}} \texorpdfstring{$\ast$}{*} \mbox{\hyperlink{group___tokenization_ga221db00549e4cd6c05c5e809b13bce26}{toknz}} (const char \texorpdfstring{$\ast$}{*}const line)
\begin{DoxyCompactList}\small\item\em Tokenizes a line (or multiple lines of code) into a set of tokens. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
Tokenization core implementation. 

Handles the conversion of source code into token streams\+:
\begin{DoxyItemize}
\item Counting tokens in source strings
\item Segmenting code into lexical units
\item Full tokenization pipeline
\end{DoxyItemize}

\begin{DoxySeeAlso}{See also}
\doxylink{lexer_8h}{lexer.\+h} For token type definitions 

\doxylink{group___token_validation}{Token Validation} For pattern checking rules 
\end{DoxySeeAlso}
